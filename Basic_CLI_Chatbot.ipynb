{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community sentence-transformers rank_bm25 ragatouille google-generativeai"
      ],
      "metadata": {
        "id": "rhSuPc_1oW4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RczwRYpzx3n8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from langchain_community.document_loaders import DataFrameLoader\n",
        "from typing import List, Optional, Tuple\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from transformers import AutoTokenizer\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain_core.runnables import ConfigurableField\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "from ragatouille import RAGPretrainedModel\n",
        "import keras_nlp\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from typing import Tuple, List, Optional\n",
        "import re\n",
        "import openai\n",
        "import os\n",
        "import time\n",
        "from IPython.display import Markdown\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"torch\" or \"tensorflow\".\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\"  # Avoid memory fragmentation on JAX backend.\n",
        "\n",
        "api_key = 'AIzaSyCJGotdlGY4Sonjve-ezUlygSfgJT1q6Mo'\n",
        "\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv('/content/drive/My Drive/RAG/rag_data.csv')\n",
        "data.head()\n",
        "data['LLM_context'] = (\n",
        "    \"Outer Key: \" + data['Outer Key'] +\n",
        "    \",\\nInner Key: \" + data['Inner Key'] +\n",
        "    \",\\nValue: \" + data['Value'] +\n",
        "    \",\\nDescription: \" + data['Descriptions']\n",
        ")\n",
        "loader = DataFrameLoader(data, page_content_column=\"LLM_context\")\n",
        "docs = loader.load()\n",
        "\n",
        "# Constants\n",
        "EMBEDDING_MODEL_NAME = \"BAAI/bge-base-en-v1.5\"\n",
        "CHUNK_SIZE = 512  # We choose a chunk size adapted to our model\n",
        "\n",
        "def split_documents(chunk_size: int, knowledge_base: List[Document], tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME) -> List[Document]:\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=int(chunk_size / 10),\n",
        "        add_start_index=True,\n",
        "        strip_whitespace=True,\n",
        "    )\n",
        "\n",
        "    docs_processed = []\n",
        "    for doc in knowledge_base:\n",
        "        docs_processed += text_splitter.split_documents([doc])\n",
        "\n",
        "    unique_texts = {}\n",
        "    docs_processed_unique = []\n",
        "    for doc in docs_processed:\n",
        "        if doc.page_content not in unique_texts:\n",
        "            unique_texts[doc.page_content] = True\n",
        "            docs_processed_unique.append(doc)\n",
        "\n",
        "    return docs_processed_unique\n",
        "\n",
        "chunked_docs = split_documents(CHUNK_SIZE, docs, tokenizer_name=EMBEDDING_MODEL_NAME)\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL_NAME,\n",
        "    multi_process=True,\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True},  # set True for cosine similarity\n",
        ")\n",
        "num_docs = 5  # Default number of documents to retrieve\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(chunked_docs).configurable_fields(\n",
        "    k=ConfigurableField(\n",
        "        id=\"search_kwargs_bm25\",\n",
        "        name=\"k\",\n",
        "        description=\"The search kwargs to use\",\n",
        "    )\n",
        ")\n",
        "\n",
        "faiss_vectorstore = FAISS.from_documents(\n",
        "    chunked_docs, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
        ")\n",
        "\n",
        "faiss_retriever = faiss_vectorstore.as_retriever(\n",
        "    search_kwargs={\"k\": num_docs}\n",
        ").configurable_fields(\n",
        "    search_kwargs=ConfigurableField(\n",
        "        id=\"search_kwargs_faiss\",\n",
        "        name=\"Search Kwargs\",\n",
        "        description=\"The search kwargs to use\",\n",
        "    )\n",
        ")\n",
        "\n",
        "vector_database = EnsembleRetriever(\n",
        "    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]  # Adjust the weight of each retriever in the EnsembleRetriever\n",
        ")\n",
        "reranker = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "prompt_template = \"\"\"\n",
        "\"When given a user query, search for the row in the data where the 'Inner Key' or the 'Descriptions' matches the query. Once you find the matching row, return the answer from the corresponding value from the 'Value' column and 'Descriptions' column\"\n",
        "\n",
        "Example:\n",
        "\n",
        "User Query: \"How to test Contactor Sequence?\"\n",
        "Model Response: \"You can test Contactor Sequence using 'V' Command. It is used for testing and simulating all contactors in a required charging sequence, the sequence simulates the contactor behavior when exiting the chargeloop.\n",
        "\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUESTION:\n",
        "{question}\n",
        "\n",
        "ANSWER:\n",
        "\"\"\"\n",
        "\n",
        "RAG_PROMPT_TEMPLATE = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=prompt_template,\n",
        ")\n",
        "\n",
        "from functools import lru_cache\n",
        "\n",
        "@lru_cache(maxsize=100)\n",
        "def cached_rerank(question, page_contents, k):\n",
        "    return reranker.rerank(question, tuple(page_contents), k=k)\n",
        "\n",
        "def answer_with_rag(question: str, knowledge_index: EnsembleRetriever, reranker: Optional[RAGPretrainedModel] = None, num_retrieved_docs: int = 5, num_docs_final: int = 5) -> Tuple[str, List[Document]]:\n",
        "    config = {\"configurable\": {\"search_kwargs_faiss\": {\"k\": num_retrieved_docs}, \"search_kwargs_bm25\": num_retrieved_docs}}\n",
        "    relevant_docs = knowledge_index.invoke(question, config=config)\n",
        "    relevant_docs = [doc.page_content for doc in relevant_docs]\n",
        "\n",
        "    if reranker:\n",
        "        relevant_docs = cached_rerank(question, tuple(relevant_docs), k=num_docs_final)\n",
        "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
        "\n",
        "    relevant_docs = relevant_docs[:num_docs_final]\n",
        "    context = relevant_docs[0] if relevant_docs else \"\"\n",
        "    final_prompt = RAG_PROMPT_TEMPLATE.format(context=context, question=question)\n",
        "\n",
        "    # Use Google's Gemini model for generation\n",
        "    system_instructions = \"You are a helpful assistant.\"\n",
        "    model_name = 'gemini-1.5-flash'  # or 'gemini-1.0-pro', 'gemini-1.5-pro'\n",
        "    temperature = 0.5\n",
        "    stop_sequence = ''\n",
        "    model = genai.GenerativeModel(model_name, system_instruction=system_instructions)\n",
        "    config = genai.GenerationConfig(temperature=temperature, stop_sequences=[stop_sequence])\n",
        "    response = model.generate_content(contents=[final_prompt], generation_config=config)\n",
        "    answer = response.text.strip()\n",
        "    print(answer)\n",
        "\n",
        "    return answer, relevant_docs\n",
        "\n",
        "def extract_keys_values(text):\n",
        "    outer_key_pattern = r'Outer Key:\\s*(.*?),'\n",
        "    inner_key_pattern = r'Inner Key:\\s*(.*?),'\n",
        "    value_pattern = r'Value:\\s*(.*?),'\n",
        "    description_pattern = r'Description:\\s*(.*)'\n",
        "\n",
        "    outer_key_match = re.search(outer_key_pattern, text)\n",
        "    inner_key_match = re.search(inner_key_pattern, text)\n",
        "    value_match = re.search(value_pattern, text)\n",
        "    description_match = re.search(description_pattern, text)\n",
        "\n",
        "    outer_key = outer_key_match.group(1).strip() if outer_key_match else None\n",
        "    inner_key = inner_key_match.group(1).strip() if inner_key_match else None\n",
        "    value = value_match.group(1).strip() if value_match else None\n",
        "    description = description_match.group(1).strip() if description_match else None\n",
        "\n",
        "    return outer_key, inner_key, value, description\n",
        "\n",
        "while True:\n",
        "    print(\"\\033[1mPlease enter your question or type 'exit' to quit:\\033[0m\", end=\" \")\n",
        "    question = input().strip()  # Change from hardcoded question to input from user\n",
        "    if question.lower() == 'exit':\n",
        "        print(\"\\033[1mGoodbye!\\033[0m\")\n",
        "        break\n",
        "\n",
        "    answer, relevant_docs = answer_with_rag(question, vector_database, reranker)\n",
        "    outer_key, inner_key, value, descriptions = extract_keys_values(answer)\n",
        "\n",
        "    display(Markdown(\"### Answer\"))\n",
        "    print(\"\\033[1mCategory:\\033[0m\", outer_key)\n",
        "    print(\"\\033[1mTask:\\033[0m\", inner_key)\n",
        "    print(\"\\033[1mCommand:\\033[0m\", value)\n",
        "    print(\"\\033[1mDescription:\\033[0m\", descriptions)\n",
        "\n"
      ]
    }
  ]
}